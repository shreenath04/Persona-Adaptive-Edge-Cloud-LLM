{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209b5600-b6b5-4428-a275-b010f26a78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "from traits_gen import call_gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edab72bf-ec63-492c-bdd1-0bbc3553f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard rule: long requests always go to the cloud model\n",
    "WORD_LENGTH_THRESHOLD = 500\n",
    "\n",
    "ROUTE_LOCAL = \"LOCAL_SMALL_MODEL\"\n",
    "ROUTE_CLOUD = \"CLOUD_LARGE_MODEL\"\n",
    "\n",
    "\n",
    "ROUTER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a router in a hybrid edge–cloud AI system.\n",
    "\n",
    "You have access to TWO models:\n",
    "\n",
    "1) LOCAL_SMALL_MODEL\n",
    "   - A 4B-parameter Gemma model running on the user's machine.\n",
    "   - Fast and cheap.\n",
    "   - This should be the DEFAULT choice in most cases.\n",
    "\n",
    "2) CLOUD_LARGE_MODEL\n",
    "   - An 8B-parameter Llama model.\n",
    "   - Slower and more expensive.\n",
    "   - Use this only when the extra capability clearly matters.\n",
    "\n",
    "Your job:\n",
    "1. Decide whether to use LOCAL_SMALL_MODEL or CLOUD_LARGE_MODEL.\n",
    "2. Build a final_prompt string that we will send directly to the chosen model.\n",
    "\n",
    "Routing principles (VERY IMPORTANT):\n",
    "\n",
    "- Start from the assumption: \"LOCAL_SMALL_MODEL is enough\".\n",
    "- Use LOCAL_SMALL_MODEL for:\n",
    "  - Short or medium-length questions.\n",
    "  - Everyday Q&A: definitions, explanations, basic reasoning, small examples.\n",
    "  - Simple coding help or small code snippets.\n",
    "  - Casual chat, jokes, preferences, simple planning.\n",
    "\n",
    "- Escalate to CLOUD_LARGE_MODEL only when you see **clear signals** that a stronger model is needed, such as:\n",
    "  - The user explicitly asks for a very detailed, exhaustive, or long answer.\n",
    "  - The task requires deep, multi-step reasoning across many components (e.g. complex system design, long essays, multi-stage plans).\n",
    "  - The user requests large code generation, complex refactoring, or analysis over a lot of text or code.\n",
    "  - The request is very long and clearly not trivial to handle with a small model.\n",
    "\n",
    "- When you are unsure or the request is borderline, prefer LOCAL_SMALL_MODEL.\n",
    "  Your goal is to minimize CLOUD_LARGE_MODEL usage while still keeping answer quality good enough for the user.\n",
    "\n",
    "The final_prompt you build should:\n",
    "- Respect persona_description, tone_preferences, expertise_level, and response_style.\n",
    "- Include clear instructions to the model about how to respond for this specific user.\n",
    "- Include the user's request verbatim somewhere.\n",
    "\n",
    "Respond ONLY as a minified JSON object with exactly these keys:\n",
    "- route: \"LOCAL_SMALL_MODEL\" or \"CLOUD_LARGE_MODEL\"\n",
    "- final_prompt: the full prompt string to send to that model\n",
    "\n",
    "No explanations, no markdown, no extra keys.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71eeae8-d5bc-40a1-b589-734c4ae8ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_code_fences(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        # remove surrounding ``` and optional language tag\n",
    "        text = text.strip(\"`\").strip()\n",
    "        text = text.replace(\"json\", \"\").replace(\"JSON\", \"\").strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67af125e-9b36-4215-99d1-644c4fe84822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_router_prompt(traits: Dict[str, Any], user_request: str) -> str:\n",
    "    payload = {\n",
    "        \"traits\": traits,\n",
    "        \"request\": user_request,\n",
    "    }\n",
    "    payload_json = json.dumps(payload, ensure_ascii=False)\n",
    "    return ROUTER_SYSTEM_PROMPT + \"\\n\\nINPUT_JSON:\\n\" + payload_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264b3dff-c17b-4993-a366-aba2161b20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_route_and_build_prompt(traits: Dict[str, Any], user_request: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Decide whether to use the local or cloud model and construct the final prompt.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"route\": \"LOCAL_SMALL_MODEL\" or \"CLOUD_LARGE_MODEL\",\n",
    "            \"final_prompt\": \"...\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    # 1) Hard length rule: very long requests → cloud, no Gemma needed\n",
    "    word_count = len(user_request.split())\n",
    "    if word_count > WORD_LENGTH_THRESHOLD:\n",
    "        # You can tune this template later\n",
    "        final_prompt = (\n",
    "            \"You are a powerful large language model.\\n\\n\"\n",
    "            f\"User persona: {traits.get('persona_description', '')}\\n\"\n",
    "            f\"Tone preferences: {traits.get('tone_preferences', '')}\\n\"\n",
    "            f\"Response style: {traits.get('response_style', '')}\\n\\n\"\n",
    "            \"User request:\\n\"\n",
    "            f\"{user_request}\"\n",
    "        )\n",
    "        return {\n",
    "            \"route\": ROUTE_CLOUD,\n",
    "            \"final_prompt\": final_prompt,\n",
    "        }\n",
    "\n",
    "    # 2) Use Gemma to decide + engineer prompt\n",
    "    prompt = _build_router_prompt(traits, user_request)\n",
    "    raw_output = call_gemma(prompt)\n",
    "    raw_output = _strip_code_fences(raw_output)\n",
    "\n",
    "    try:\n",
    "        data = json.loads(raw_output)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: if Gemma messes up, default to local with a simple prompt\n",
    "        final_prompt = (\n",
    "            \"You are an AI assistant.\\n\\n\"\n",
    "            f\"User persona: {traits.get('persona_description', '')}\\n\"\n",
    "            f\"Tone preferences: {traits.get('tone_preferences', '')}\\n\"\n",
    "            f\"Response style: {traits.get('response_style', '')}\\n\\n\"\n",
    "            \"User request:\\n\"\n",
    "            f\"{user_request}\"\n",
    "        )\n",
    "        return {\n",
    "            \"route\": ROUTE_LOCAL,\n",
    "            \"final_prompt\": final_prompt,\n",
    "        }\n",
    "\n",
    "    route = data.get(\"route\", ROUTE_LOCAL)\n",
    "    final_prompt = data.get(\"final_prompt\", \"\")\n",
    "\n",
    "    # Small safety: normalize route\n",
    "    if route not in {ROUTE_LOCAL, ROUTE_CLOUD}:\n",
    "        route = ROUTE_LOCAL\n",
    "\n",
    "    if not final_prompt:\n",
    "        # Fallback if Gemma forgot to fill it\n",
    "        final_prompt = (\n",
    "            \"You are an AI assistant.\\n\\n\"\n",
    "            f\"User persona: {traits.get('persona_description', '')}\\n\"\n",
    "            f\"Tone preferences: {traits.get('tone_preferences', '')}\\n\"\n",
    "            f\"Response style: {traits.get('response_style', '')}\\n\\n\"\n",
    "            \"User request:\\n\"\n",
    "            f\"{user_request}\"\n",
    "        )\n",
    "\n",
    "    return {\"route\": route, \"final_prompt\": final_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537560c4-bf69-4d7a-b18a-0a6f7d29049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 1 ROUTE: LOCAL_SMALL_MODEL\n",
      "\n",
      "Prompt Sent To Model:\n",
      " Okay, let's tackle recursion! Here's an explanation, tailored for someone like you who appreciates step-by-step clarity. \n",
      "\n",
      "Recursion is a powerful programming technique where a function calls itself within its own definition. Think of it like a set of Russian nesting dolls – each doll contains a smaller version of itself. In programming, a recursive function breaks down a problem into smaller, sel ...\n",
      "\n",
      "\n",
      "TEST 2 ROUTE: LOCAL_SMALL_MODEL\n",
      "\n",
      "Prompt Sent To Model:\n",
      " Okay, let's build a comprehensive guide for a distributed system with microservices. You asked for it repeatedly, so here’s a detailed breakdown. As a CS student who appreciates step-by-step explanations, I'll focus on clarity and practical considerations. \n",
      "\n",
      "**Distributed System with Microservices: A Detailed Guide**\n",
      "\n",
      "1.  **Core Concepts:**\n",
      "    *   **Microservices:** Small, independent services communicating over lightweight mechanisms (e.g., REST, gRPC, message queues). This allows for independent scaling and development.\n",
      "    *   **Distributed Systems:** Systems where components are located on networked computers, requiring careful coordination and fault tolerance.\n",
      "\n",
      "2.  **Architecture Components:**\n",
      "    *   **Load Balancing:** Distributes incoming traffic across multiple instances of microservices to prevent overload and ensure high availability. (e.g., Nginx, HAProxy)\n",
      "    *   **Autoscaling:** Automatically adjusts the number of instances of a microservice based on demand. (e.g., Kubernetes HPA, AWS Auto Scaling)\n",
      "    *   **Observability:** Monitoring, logging, and tracing to understand the behavior of the system and identify issues. (e.g., Prometheus, Grafana, Jaeger)\n",
      "    *   **CI/CD:** Continuous Integration/Continuous Deployment pipelines for automated building, testing, and deployment.\n",
      "    *   **Message Queues:** Enable asynchronous communication between microservices. (e.g., Kafka, RabbitMQ)\n",
      "\n",
      "3.  **Technology Choices:**\n",
      "    *   **Programming Languages:** Java, Python, Go, Node.js\n",
      "    *   **Containerization:** Docker\n",
      "    *   **Orchestration:** Kubernetes\n",
      "    *   **Databases:** PostgreSQL, MongoDB\n",
      "\n",
      "4.  **Key Considerations:**\n",
      "    *   **Service Discovery:** Mechanisms for microservices to find and communicate with each other (e.g., Consul, etcd)\n",
      "    *   **Circuit Breakers:** Prevent cascading failures by isolating faulty services.\n",
      "    *   **Data Consistency:** Strategies for maintaining data consistency across distributed databases.\n",
      "\n",
      "This is a high-level overview. Each component deserves significant detailed exploration. Do you want me to elaborate on any specific aspect, such as the implementation of a CI/CD pipeline or a detailed explanation of message queue concepts? ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fake traits for testing\n",
    "traits = {\n",
    "    \"persona_description\": \"A CS student who likes clear, step-by-step explanations.\",\n",
    "    \"tone_preferences\": \"friendly and detailed\",\n",
    "    \"expertise_level\": \"intermediate\",\n",
    "    \"preferred_language\": \"en\",\n",
    "    \"response_style\": \"use examples\"\n",
    "}\n",
    "\n",
    "# --- Test 1: Simple request ---\n",
    "user_request_1 = \"Explain what is recursion in one paragraph.\"\n",
    "route_info_1 = decide_route_and_build_prompt(traits, user_request_1)\n",
    "\n",
    "print(\"TEST 1 ROUTE:\", route_info_1[\"route\"])\n",
    "print(\"\\nPrompt Sent To Model:\\n\", route_info_1[\"final_prompt\"][:400], \"...\")\n",
    "\n",
    "# --- Test 2: Complex / Long request ---\n",
    "user_request_2 = (\n",
    "    \"Write a full detailed guide for building a distributed system with microservices, \"\n",
    "    \"including load balancing, autoscaling, observability, CI/CD, message queues...\"\n",
    "    * 20  # artificially amplify length to force cloud behavior\n",
    ")\n",
    "\n",
    "route_info_2 = decide_route_and_build_prompt(traits, user_request_2)\n",
    "\n",
    "print(\"\\n\\nTEST 2 ROUTE:\", route_info_2[\"route\"])\n",
    "print(\"\\nPrompt Sent To Model:\\n\", route_info_2[\"final_prompt\"], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cf2fc-7d24-446b-b3d7-93704b51fc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edgecloud-project",
   "language": "python",
   "name": "edgecloud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
